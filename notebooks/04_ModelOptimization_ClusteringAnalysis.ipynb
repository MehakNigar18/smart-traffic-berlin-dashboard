{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf3cc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Traffic Data Clustering - GMM with Hyperparameter Tuning \n",
    "# ----------------------------------------\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Import the model training function from your model file\n",
    "from model.model import train_gmm_model  # Assuming model.py is in the model folder\n",
    "\n",
    "# -------------------------------\n",
    "# Step 0: Initialize Spark Session\n",
    "# -------------------------------\n",
    "spark = SparkSession.builder.appName(\"TrafficDataModelOptimization\").getOrCreate()\n",
    "\n",
    "# Disable Arrow Optimization in Spark to prevent issues with VectorUDT conversion\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load Data\n",
    "# -------------------------------\n",
    "input_path = \"dbfs:/user/mehak/processed/berlin_clean.csv\"\n",
    "spark_df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "print(\"✅ Data Loaded Successfully!\")\n",
    "spark_df.show(5)\n",
    "spark_df.printSchema()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Drop any old feature columns (Safety Check)\n",
    "# -------------------------------\n",
    "# Ensure any previous features are dropped to avoid conflict\n",
    "for col in ['features', 'scaledFeatures', 'input_features', 'scaled_features']:\n",
    "    if col in spark_df.columns:\n",
    "        spark_df = spark_df.drop(col)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Use External Model Code to Train the GMM Model\n",
    "# -------------------------------\n",
    "# Call the train_gmm_model function from your model.py file to train the model\n",
    "gmm_model = train_gmm_model(spark_df, k=5)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Apply the Model and Generate Predictions\n",
    "# -------------------------------\n",
    "gmm_result = gmm_model.transform(spark_df)\n",
    "\n",
    "print(\"✅ GMM Model Fitted Successfully!\")\n",
    "gmm_result.select(\"name\", \"berlin_bez\", \"prediction\").show(10)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Manual Hyperparameter Tuning using Silhouette Score\n",
    "# -------------------------------\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", predictionCol=\"prediction\")\n",
    "best_k = None\n",
    "best_score = -1\n",
    "silhouette_scores = []\n",
    "\n",
    "k_range = [2, 3, 4, 5, 6, 7]\n",
    "for k_val in k_range:\n",
    "    gmm_temp = GaussianMixture(k=k_val, featuresCol=\"scaled_features\", predictionCol=\"prediction\")\n",
    "    pipeline_temp = Pipeline(stages=indexers_high + [assembler, scaler, gmm_temp])\n",
    "    model_temp = pipeline_temp.fit(spark_df)\n",
    "    result_temp = model_temp.transform(spark_df)\n",
    "    score = evaluator.evaluate(result_temp)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"k={k_val}, Silhouette Score = {score}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k_val\n",
    "\n",
    "print(f\"✅ Best k based on Silhouette Score: {best_k}\")\n",
    "\n",
    "# Plot Silhouette Scores for different k values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Score for Different k (GMM)\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: PCA for Visualization (3D)\n",
    "# -------------------------------\n",
    "# Convert scaled_features to array\n",
    "to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "cleaned_df = gmm_result.withColumn(\"scaledArray\", to_array_udf(\"scaled_features\"))\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(k=3, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(cleaned_df)\n",
    "pca_df = pca_model.transform(cleaned_df)\n",
    "\n",
    "# Collect PCA data into Pandas\n",
    "plot_df = pca_df.select(\"pca_features\", \"prediction\").rdd.map(lambda row: (row['pca_features'], row['prediction'])).toDF([\"pcaArray\", \"prediction\"]).toPandas()\n",
    "\n",
    "# Extract x, y, z for 3D plotting\n",
    "plot_df[\"x\"] = plot_df[\"pcaArray\"].apply(lambda x: float(x[0]))\n",
    "plot_df[\"y\"] = plot_df[\"pcaArray\"].apply(lambda x: float(x[1]))\n",
    "plot_df[\"z\"] = plot_df[\"pcaArray\"].apply(lambda x: float(x[2]))\n",
    "\n",
    "# 3D Plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for cluster_id in plot_df[\"prediction\"].unique():\n",
    "    subset = plot_df[plot_df[\"prediction\"] == cluster_id]\n",
    "    ax.scatter(subset[\"x\"], subset[\"y\"], subset[\"z\"], label=f\"Cluster {cluster_id}\", alpha=0.6)\n",
    "ax.set_xlabel('PCA Feature 1')\n",
    "ax.set_ylabel('PCA Feature 2')\n",
    "ax.set_zlabel('PCA Feature 3')\n",
    "ax.set_title(\"GMM Clusters (3D PCA Projection)\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Cluster Summary\n",
    "# -------------------------------\n",
    "cluster_summary = gmm_result.groupBy(\"prediction\").mean(\"zahl_tvz\", \"vz_typ_no\", \"lor_prg\")\n",
    "cluster_summary.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 8: Visualize Cluster Characteristics\n",
    "# -------------------------------\n",
    "pdf = gmm_result.toPandas()\n",
    "\n",
    "sns.boxplot(x=\"prediction\", y=\"zahl_tvz\", data=pdf)\n",
    "plt.title(\"Traffic Volume Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x=\"prediction\", y=\"vz_typ_no\", data=pdf)\n",
    "plt.title(\"Vehicle Type Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n",
    "gmm_result.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"traffic_databricks_ws.default.traffic_data_gmm_clusters\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
