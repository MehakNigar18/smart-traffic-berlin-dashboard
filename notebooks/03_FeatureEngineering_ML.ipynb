{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d53760",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"TrafficDataML\").getOrCreate()\n",
    "\n",
    "# Disable Arrow Optimization in Spark to prevent issues with VectorUDT conversion\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "# Load data\n",
    "input_path = \"dbfs:/user/mehak/processed/berlin_clean.csv\"\n",
    "spark_df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show data and schema\n",
    "spark_df.show(5)\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Define categorical and numeric columns\n",
    "categorical_low = [\"spatial_type\"]  # One-Hot Encoding\n",
    "categorical_high = [\"name\", \"berlin_bez\"]  # Label Encoding\n",
    "numeric_cols = [\"zahl_tvz\", \"vz_typ_no\", \"lor_prg\"]  # Adjust based on your dataset\n",
    "\n",
    "# Encoding Categorical Features\n",
    "indexers_high = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_high]\n",
    "indexers_low = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_low]\n",
    "encoders_low = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\") for col in categorical_low]\n",
    "\n",
    "# Assemble features\n",
    "assembler_inputs = numeric_cols + [col+\"_index\" for col in categorical_high] + [col+\"_vec\" for col in categorical_low]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "# KMeans Model Initialization\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\", predictionCol=\"cluster\", k=5, seed=42)\n",
    "\n",
    "# Build pipeline with all stages (encoding, assembling, scaling, clustering)\n",
    "stages = indexers_high + indexers_low + encoders_low + [assembler, scaler, kmeans]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit model and transform the data\n",
    "model = pipeline.fit(spark_df)\n",
    "clustered_df = model.transform(spark_df)\n",
    "\n",
    "# Show the first 10 cluster assignments\n",
    "clustered_df.select(\"name\", \"spatial_type\", \"cluster\").show(10)\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Step 1: Clustering Metrics & Centers\n",
    "# -------------------------------\n",
    "kmeans_model = model.stages[-1]  # Last stage is KMeans\n",
    "wssse = kmeans_model.summary.trainingCost  # Within Set Sum of Squared Errors (WSSSE)\n",
    "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")\n",
    "\n",
    "# Cluster Centers\n",
    "centers = kmeans_model.clusterCenters()\n",
    "print(\"Cluster Centers:\")\n",
    "for idx, center in enumerate(centers):\n",
    "    print(f\"Cluster {idx}: {center}\")\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Step 2: Silhouette Score for Clustering Quality\n",
    "# -------------------------------\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.setPredictionCol('cluster').evaluate(clustered_df)\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Step 3: PCA for Visualization (Dimensionality Reduction to 2D)\n",
    "# -------------------------------\n",
    "# Convert the vector columns (e.g., scaledFeatures) to an array before converting to Pandas\n",
    "to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "cleaned_df = clustered_df.withColumn(\"pcaArray\", to_array_udf(\"scaledFeatures\"))\n",
    "\n",
    "# Apply PCA to reduce scaled features to 2D\n",
    "pca = PCA(k=2, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(cleaned_df)\n",
    "pca_df = pca_model.transform(cleaned_df)\n",
    "\n",
    "# Collect data to Pandas using .rdd (avoid Arrow optimization)\n",
    "plot_df = pca_df.select(\"pcaFeatures\", \"cluster\").rdd.map(lambda row: (row['pcaFeatures'], row['cluster'])).toDF([\"pcaArray\", \"cluster\"]).toPandas()\n",
    "\n",
    "# Extract x, y from PCA array\n",
    "plot_df[\"x\"] = plot_df[\"pcaArray\"].apply(lambda x: float(x[0]))\n",
    "plot_df[\"y\"] = plot_df[\"pcaArray\"].apply(lambda x: float(x[1]))\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster_id in plot_df[\"cluster\"].unique():\n",
    "    subset = plot_df[plot_df[\"cluster\"] == cluster_id]\n",
    "    plt.scatter(subset[\"x\"], subset[\"y\"], label=f\"Cluster {cluster_id}\", alpha=0.6)\n",
    "plt.title(\"KMeans Clusters (PCA 2D Projection)\")\n",
    "plt.xlabel(\"PCA Feature 1\")\n",
    "plt.ylabel(\"PCA Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Step 4: Elbow Method for Optimal k (Selecting k based on WSSSE)\n",
    "# -------------------------------\n",
    "wssse_list = []\n",
    "k_range = range(2, 11)  # Testing k from 2 to 10 clusters\n",
    "for k_val in k_range:\n",
    "    kmeans.setK(k_val)  # Set k value\n",
    "    model = pipeline.fit(spark_df)  # Fit model with new k\n",
    "    clustered_df = model.transform(spark_df)  # Apply to data\n",
    "    wssse_list.append(model.stages[-1].summary.trainingCost)  # Collect WSSSE for each k\n",
    "\n",
    "# Plot Elbow Method to visualize the best k\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, wssse_list, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WSSSE\")\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Step 5: Cluster Analysis (Mean Values of Features by Cluster)\n",
    "# -------------------------------\n",
    "# Compute the mean values for each feature per cluster\n",
    "cluster_summary = clustered_df.groupBy(\"cluster\").mean(\"zahl_tvz\", \"vz_typ_no\", \"lor_prg\")\n",
    "cluster_summary.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Step 6: Visualize Cluster Characteristics (Traffic Volume and Vehicle Type Distribution)\n",
    "# -------------------------------\n",
    "# Plot the distribution of traffic volume by cluster\n",
    "sns.boxplot(x=\"cluster\", y=\"zahl_tvz\", data=clustered_df.toPandas())\n",
    "plt.title(\"Traffic Volume Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of vehicle type by cluster\n",
    "sns.boxplot(x=\"cluster\", y=\"vz_typ_no\", data=clustered_df.toPandas())\n",
    "plt.title(\"Vehicle Type Distribution by Cluster\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
