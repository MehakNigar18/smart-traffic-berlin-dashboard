{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c29cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml.clustering import GaussianMixture, KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"TrafficDataML_Refinement\").getOrCreate()\n",
    "\n",
    "# Disable Arrow Optimization in Spark to prevent issues with VectorUDT conversion\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "# Load data\n",
    "input_path = \"dbfs:/user/mehak/processed/berlin_clean.csv\"\n",
    "spark_df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show data and schema\n",
    "spark_df.show(5)\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Define categorical and numeric columns\n",
    "categorical_low = [\"spatial_type\"]  # One-Hot Encoding\n",
    "categorical_high = [\"name\", \"berlin_bez\"]  # Label Encoding\n",
    "numeric_cols = [\"zahl_tvz\", \"vz_typ_no\", \"lor_prg\"]  # Adjust based on your dataset\n",
    "\n",
    "# Encoding Categorical Features\n",
    "indexers_high = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_high]\n",
    "indexers_low = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_low]\n",
    "encoders_low = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\") for col in categorical_low]\n",
    "\n",
    "# Assemble features\n",
    "assembler_inputs = numeric_cols + [col+\"_index\" for col in categorical_high] + [col+\"_vec\" for col in categorical_low]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Gaussian Mixture Model (GMM) for Clustering\n",
    "# -------------------------------\n",
    "gmm = GaussianMixture(k=5, featuresCol=\"scaledFeatures\", predictionCol=\"prediction\")  # Note: 'prediction' instead of 'cluster'\n",
    "\n",
    "# Build pipeline for GMM\n",
    "gmm_pipeline = Pipeline(stages=indexers_high + indexers_low + encoders_low + [assembler, scaler, gmm])\n",
    "\n",
    "# Fit GMM model\n",
    "gmm_model = gmm_pipeline.fit(spark_df)\n",
    "gmm_result = gmm_model.transform(spark_df)\n",
    "\n",
    "# Show the first 10 cluster assignments for GMM\n",
    "gmm_result.select(\"name\", \"spatial_type\", \"prediction\").show(10)  # Note: 'prediction' instead of 'cluster'\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Evaluate GMM with Silhouette Score\n",
    "# -------------------------------\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"prediction\")  # Note: 'prediction' instead of 'cluster'\n",
    "silhouette = evaluator.evaluate(gmm_result)\n",
    "print(f\"GMM Silhouette Score: {silhouette}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: 3D PCA for Visualization (Dimensionality Reduction)\n",
    "# -------------------------------\n",
    "# Convert vector columns (e.g., scaledFeatures) to array before plotting\n",
    "to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "cleaned_df = gmm_result.withColumn(\"pcaArray\", to_array_udf(\"scaledFeatures\"))\n",
    "\n",
    "# Apply PCA to reduce scaled features to 3D\n",
    "pca = PCA(k=3, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(cleaned_df)\n",
    "pca_df = pca_model.transform(cleaned_df)\n",
    "\n",
    "# Collect PCA data into Pandas for visualization\n",
    "plot_df = pca_df.select(\"pcaFeatures\", \"prediction\").rdd.map(lambda row: (row['pcaFeatures'], row['prediction'])).toDF([\"pcaArray\", \"prediction\"]).toPandas()\n",
    "\n",
    "# Extract x, y, z from PCA array for 3D plotting\n",
    "plot_df[\"x\"] = plot_df[\"pcaArray\"].apply(lambda x: float(x[0]))\n",
    "plot_df[\"y\"] = plot_df[\"pcaArray\"].apply(lambda x: float(x[1]))\n",
    "plot_df[\"z\"] = plot_df[\"pcaArray\"].apply(lambda x: float(x[2]))\n",
    "\n",
    "# Plotting 3D PCA\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for cluster_id in plot_df[\"prediction\"].unique():  # Note: 'prediction' instead of 'cluster'\n",
    "    subset = plot_df[plot_df[\"prediction\"] == cluster_id]\n",
    "    ax.scatter(subset[\"x\"], subset[\"y\"], subset[\"z\"], label=f\"Cluster {cluster_id}\", alpha=0.6)\n",
    "ax.set_xlabel('PCA Feature 1')\n",
    "ax.set_ylabel('PCA Feature 2')\n",
    "ax.set_zlabel('PCA Feature 3')\n",
    "ax.set_title(\"GMM Clusters (3D PCA Projection)\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Elbow Method for Optimal k (Selecting k based on WSSSE for GMM)\n",
    "# -------------------------------\n",
    "wssse_list = []\n",
    "k_range = range(2, 11)  # Testing k from 2 to 10 clusters for GMM\n",
    "for k_val in k_range:\n",
    "    gmm.setK(k_val)  # Set k value\n",
    "    gmm_model = gmm_pipeline.fit(spark_df)  # Fit model with new k\n",
    "    gmm_result = gmm_model.transform(spark_df)  # Apply to data\n",
    "    evaluator = ClusteringEvaluator(predictionCol=\"prediction\")  # Note: 'prediction' instead of 'cluster'\n",
    "    wssse_list.append(evaluator.evaluate(gmm_result))  # Collect WSSSE for each k\n",
    "\n",
    "# Plot Elbow Method to visualize the best k for GMM\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, wssse_list, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal k (GMM)\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WSSSE\")\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Cluster Analysis (Mean Values of Features by Cluster)\n",
    "# -------------------------------\n",
    "# Compute the mean values for each feature per cluster\n",
    "cluster_summary = gmm_result.groupBy(\"prediction\").mean(\"zahl_tvz\", \"vz_typ_no\", \"lor_prg\")  # Note: 'prediction' instead of 'cluster'\n",
    "cluster_summary.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Visualize Cluster Characteristics (Traffic Volume and Vehicle Type Distribution)\n",
    "# -------------------------------\n",
    "# Plot the distribution of traffic volume by cluster\n",
    "sns.boxplot(x=\"prediction\", y=\"zahl_tvz\", data=gmm_result.toPandas())  # Note: 'prediction' instead of 'cluster'\n",
    "plt.title(\"Traffic Volume Distribution by Cluster (GMM)\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of vehicle type by cluster\n",
    "sns.boxplot(x=\"prediction\", y=\"vz_typ_no\", data=gmm_result.toPandas())  # Note: 'prediction' instead of 'cluster'\n",
    "plt.title(\"Vehicle Type Distribution by Cluster (GMM)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
