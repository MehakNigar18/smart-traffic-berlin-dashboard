{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ddcf7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸš€ Notebook 06: Model Deployment & Azure Integration\n",
    "\n",
    "# 1. Import Required Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "# 2. Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"ModelDeploymentAzureIntegration\").getOrCreate()\n",
    "\n",
    "# 3. Configure Azure Blob Storage Access Key (NO secrets used)\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mehaktrafficstore.blob.core.windows.net\",\n",
    "    \"9ri/YNuCzOtJ+naNRqgt9HcG6J4q6MK5ef7/ubSH6N5HZqz+gvRrvrp4EqwvURZbpDfbv6B1lLmR+AStjBwedA==\"\n",
    ")\n",
    "\n",
    "# 4. Load Cleaned Dataset\n",
    "input_path = \"dbfs:/user/mehak/processed/berlin_clean.csv\"\n",
    "spark_df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "# 5. Define Feature Columns\n",
    "categorical_low = [\"spatial_type\"]         # One-hot encoding\n",
    "categorical_high = [\"name\", \"berlin_bez\"]  # Label encoding\n",
    "numeric_cols = [\"zahl_tvz\", \"vz_typ_no\", \"lor_prg\"]\n",
    "\n",
    "\n",
    "# 6. Preprocessing Pipeline for GMM\n",
    "indexers_high = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_high]\n",
    "indexers_low = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_low]\n",
    "encoders_low = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vec\") for col in categorical_low]\n",
    "\n",
    "assembler_inputs = numeric_cols + [col + \"_index\" for col in categorical_high] + [col + \"_vec\" for col in categorical_low]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "# 7. GMM Clustering\n",
    "gmm = GaussianMixture(k=5, featuresCol=\"scaledFeatures\", predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=indexers_high + indexers_low + encoders_low + [assembler, scaler, gmm])\n",
    "gmm_model = pipeline.fit(spark_df)\n",
    "gmm_result = gmm_model.transform(spark_df)\n",
    "\n",
    "# 8. Select Final Columns for Export\n",
    "export_df = gmm_result.select(\"name\", \"spatial_type\", \"zahl_tvz\", \"vz_typ_no\", \"lor_prg\", \"prediction\")\n",
    "export_df.show(5)\n",
    "\n",
    "# 9. Define Azure Blob Storage Output Paths\n",
    "azure_output_path_parquet = \"wasbs://traffic-data@mehaktrafficstore.blob.core.windows.net/final_cluster_results_parquet\"\n",
    "azure_output_path_csv = \"wasbs://traffic-data@mehaktrafficstore.blob.core.windows.net/final_cluster_results_csv\"\n",
    "\n",
    "# 10. Save to Azure Blob Storage\n",
    "# Save as Parquet (recommended for Power BI)\n",
    "export_df.write.mode(\"overwrite\").parquet(azure_output_path_parquet)\n",
    "\n",
    "# Save as CSV (optional, Power BI also supports CSV)\n",
    "export_df.write.mode(\"overwrite\").option(\"header\", True).csv(azure_output_path_csv)\n",
    "\n",
    "# 11. Save Local Copy to DBFS for Download (as CSV)\n",
    "# Ensure the local directory exists\n",
    "os.makedirs(\"/dbfs/FileStore/tables/\", exist_ok=True)\n",
    "\n",
    "# Convert to Pandas and Save\n",
    "pandas_df = export_df.toPandas()\n",
    "pandas_df.to_csv(\"/dbfs/FileStore/tables/final_cluster_results.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Export completed: Azure + DBFS local CSV ready.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
